{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KyleBot: A GPT-2 Chatbot for Learning\n",
    "\n",
    "This notebook demonstrates how to build a generative AI chatbot using GPT-2. Each section includes educational comments to help you understand what's happening.\n",
    "\n",
    "## What You'll Learn:\n",
    "- How to load and use pre-trained language models\n",
    "- Different text generation strategies (greedy, beam search, sampling)\n",
    "- How to create a conversational interface\n",
    "- Parameter tuning for better responses\n",
    "- Building an interactive chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model and tokenizer...\n",
      "‚úÖ GPT-2 loaded successfully!\n",
      "Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Load Model\n",
    "# This cell sets up everything we need for our chatbot\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "# The tokenizer converts text to numbers that the model can understand\n",
    "# The model contains the learned weights from training on massive amounts of text\n",
    "print(\"Loading GPT-2 model and tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the model to evaluation mode (no training)\n",
    "model.eval()\n",
    "\n",
    "# Add a special token for the end of text\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ GPT-2 loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_greedy(prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]\n",
    "\n",
    "def generate_response_sampling(prompt, max_new_tokens=50, temperature=0.8, top_k=50):\n",
    "    \"\"\"\n",
    "    Sampling with temperature and top-k: More creative and diverse responses\n",
    "    - temperature: Controls randomness (higher = more random)\n",
    "    - top_k: Only considers the top k most likely words\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]\n",
    "\n",
    "def generate_response_beam_search(prompt, max_new_tokens=50, num_beams=5):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=num_beams,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KyleBot created and ready to chat!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Chatbot Class\n",
    "# This class organizes our chatbot functionality\n",
    "\n",
    "class KyleBot:\n",
    "    def __init__(self, name=\"KyleBot\"):\n",
    "        self.name = name\n",
    "        self.conversation_history = []\n",
    "        self.generation_method = \"sampling\"  # Default method\n",
    "        \n",
    "    def add_to_history(self, user_input, bot_response):\n",
    "        \"\"\"Keep track of conversation for context\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"user\": user_input,\n",
    "            \"bot\": bot_response\n",
    "        })\n",
    "        \n",
    "    def create_context_prompt(self, user_input):\n",
    "        \"\"\"Create a prompt with conversation history for context\"\"\"\n",
    "        prompt = f\"You are {self.name}, a knowledgeable AI. Provide a clear and concise definition of the user's topic.\\n\"\n",
    "        if len(self.conversation_history) > 0:\n",
    "            recent_history = self.conversation_history[-2:]\n",
    "            context = \"\\n\".join([\n",
    "                f\"User: {exchange['user']}\\n{self.name}: {exchange['bot']}\"\n",
    "                for exchange in recent_history\n",
    "            ])\n",
    "            prompt += context + \"\\n\"\n",
    "        prompt += f\"User: {user_input}\\n{self.name}: \"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_response(self, user_input, method=None, **kwargs):\n",
    "        \"\"\"Generate a response using the specified method\"\"\"\n",
    "        method = method or self.generation_method\n",
    "        prompt = self.create_context_prompt(user_input)\n",
    "        print(f\"Using method: {method}, Prompt: {prompt}\")  # Debug\n",
    "    \n",
    "        if method == \"greedy\":\n",
    "            response = generate_response_greedy(prompt, **kwargs)\n",
    "        elif method == \"sampling\":\n",
    "            response = generate_response_sampling(prompt, **kwargs)\n",
    "        elif method == \"beam\":\n",
    "            response = generate_response_beam_search(prompt, **kwargs)\n",
    "        else:\n",
    "            response = generate_response_sampling(prompt, **kwargs)\n",
    "    \n",
    "        response = self.clean_response(response)\n",
    "        self.add_to_history(user_input, response)\n",
    "        return response\n",
    "    \n",
    "    def clean_response(self, response, max_chars=500):\n",
    "        \"\"\"Clean up the generated response\"\"\"\n",
    "        response = response.strip()\n",
    "        response = re.sub(rf\"^{self.name}:\\s*|^User:\\s*|\\n{self.name}:.*\", \"\", response)\n",
    "        if len(response) > max_chars:\n",
    "            for char in ['.', '!', '?']:\n",
    "                if char in response[:max_chars]:\n",
    "                    response = response[:response.index(char) + 1]\n",
    "                    break\n",
    "        return response\n",
    "    \n",
    "    def set_generation_method(self, method):\n",
    "        \"\"\"Change the generation method\"\"\"\n",
    "        valid_methods = [\"greedy\", \"sampling\", \"beam\"]\n",
    "        if method in valid_methods:\n",
    "            self.generation_method = method\n",
    "            print(f\"‚úÖ Generation method set to: {method}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Invalid method. Choose from: {valid_methods}\")\n",
    "\n",
    "# Create our chatbot instance\n",
    "kylebot = KyleBot()\n",
    "print(\"‚úÖ KyleBot created and ready to chat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Testing different generation methods:\n",
      "==================================================\n",
      "\n",
      "1. GREEDY DECODING (always picks most likely word):\n",
      "Using method: greedy, Prompt: You are KyleBot, a knowledgeable AI. Provide a clear and concise definition of the user's topic.\n",
      "User: What is artificial intelligence?\n",
      "KyleBot: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_response_greedy() got an unexpected keyword argument 'no_repeat_ngram_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Test greedy decoding\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. GREEDY DECODING (always picks most likely word):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mkylebot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgreedy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Test sampling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mKyleBot.generate_response\u001b[0;34m(self, user_input, method, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debug\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     39\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_response_sampling(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_response_greedy() got an unexpected keyword argument 'no_repeat_ngram_size'"
     ]
    }
   ],
   "source": [
    "test_prompt = \"What is artificial intelligence?\"\n",
    "\n",
    "print(\"ü§ñ Testing different generation methods:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test greedy decoding\n",
    "print(\"\\n1. GREEDY DECODING (always picks most likely word):\")\n",
    "response = kylebot.generate_response(test_prompt, method=\"greedy\", max_new_tokens=100, no_repeat_ngram_size=2, repetition_penalty=1.2, max_chars=500)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test sampling\n",
    "print(\"\\n2. SAMPLING (more creative, uses temperature and top-k):\")\n",
    "response = kylebot.generate_response(test_prompt, method=\"sampling\", max_new_tokens=100, temperature=0.7, top_k=30, max_chars=500)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Test beam search\n",
    "print(\"\\n3. BEAM SEARCH (explores multiple possibilities):\")\n",
    "response = kylebot.generate_response(test_prompt, method=\"beam\", max_new_tokens=100, num_beams=5, no_repeat_ngram_size=2, repetition_penalty=1.2, max_chars=500)\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üí° Notice how each method produces different styles of responses!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Interactive Chat Loop\n",
    "# This is where you can actually chat with KyleBot!\n",
    "\n",
    "def chat_with_kylebot():\n",
    "    \"\"\"Interactive chat interface\"\"\"\n",
    "    print(\"ü§ñ Welcome to KyleBot! Let's chat!\")\n",
    "    print(\"üí° Commands:\")\n",
    "    print(\"   - Type 'quit' to exit\")\n",
    "    print(\"   - Type 'method: [greedy/sampling/beam]' to change generation method\")\n",
    "    print(\"   - Type 'history' to see conversation history\")\n",
    "    print(\"   - Type 'help' for this message\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nüë§ You: \").strip()\n",
    "            \n",
    "            # Handle special commands\n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"üëã Goodbye! Thanks for chatting with KyleBot!\")\n",
    "                break\n",
    "            elif user_input.lower() == 'help':\n",
    "                print(\"üí° Commands:\")\n",
    "                print(\"   - Type 'quit' to exit\")\n",
    "                print(\"   - Type 'method: [greedy/sampling/beam]' to change generation method\")\n",
    "                print(\"   - Type 'history' to see conversation history\")\n",
    "                print(\"   - Type 'help' for this message\")\n",
    "                continue\n",
    "            elif user_input.lower() == 'history':\n",
    "                if kylebot.conversation_history:\n",
    "                    print(\"\\nüìú Conversation History:\")\n",
    "                    for i, exchange in enumerate(kylebot.conversation_history, 1):\n",
    "                        print(f\"{i}. You: {exchange['user']}\")\n",
    "                        print(f\"   KyleBot: {exchange['bot']}\")\n",
    "                else:\n",
    "                    print(\"üìú No conversation history yet.\")\n",
    "                continue\n",
    "            elif user_input.lower().startswith('method:'):\n",
    "                method = user_input.split(':')[1].strip()\n",
    "                kylebot.set_generation_method(method)\n",
    "                continue\n",
    "            elif not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Generate and display response\n",
    "            print(f\"\\nü§ñ KyleBot ({kylebot.generation_method}): \", end=\"\")\n",
    "            response = kylebot.generate_response(user_input)\n",
    "            print(response)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Goodbye! Thanks for chatting with KyleBot!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            print(\"Try again or type 'quit' to exit.\")\n",
    "\n",
    "# Uncomment the line below to start chatting!\n",
    "chat_with_kylebot()\n",
    "\n",
    "print(\"üí° To start chatting, uncomment the 'chat_with_kylebot()' line above and run this cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Experiment with Parameters\n",
    "# This cell helps you understand how different parameters affect generation\n",
    "\n",
    "def experiment_with_parameters():\n",
    "    \"\"\"Demonstrate how different parameters affect text generation\"\"\"\n",
    "    \n",
    "    test_prompt = \"The future of technology is\"\n",
    "    \n",
    "    print(\"üß™ Parameter Experimentation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test different temperatures\n",
    "    print(\"\\nüå°Ô∏è  Temperature Effect (controls randomness):\")\n",
    "    temperatures = [0.1, 0.5, 1.0, 1.5]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        response = generate_response_sampling(\n",
    "            test_prompt, \n",
    "            max_new_tokens=50, \n",
    "            temperature=temp, \n",
    "            top_k=50\n",
    "        )\n",
    "        print(f\"Temperature {temp}: {response}\")\n",
    "    \n",
    "    # Test different top-k values\n",
    "    print(\"\\nüîù Top-k Effect (limits word choices):\")\n",
    "    top_k_values = [10, 20, 50, 100]\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        response = generate_response_sampling(\n",
    "            test_prompt, \n",
    "            max_new_tokens=50, \n",
    "            temperature=0.8, \n",
    "            top_k=k\n",
    "        )\n",
    "        print(f\"Top-k {k}: {response}\")\n",
    "    \n",
    "    # Test different beam search beam counts\n",
    "    print(\"\\nüîç Beam Search Effect (number of parallel searches):\")\n",
    "    beam_counts = [1, 3, 5, 10]\n",
    "    \n",
    "    for beams in beam_counts:\n",
    "        response = generate_response_beam_search(\n",
    "            test_prompt, \n",
    "            max_new_tokens=50\n",
    "            , \n",
    "            num_beams=beams\n",
    "        )\n",
    "        print(f\"Beams {beams}: {response}\")\n",
    "\n",
    "# Run the experiment\n",
    "experiment_with_parameters()\n",
    "\n",
    "print(\"\\nüí° Key Takeaways:\")\n",
    "print(\"   - Lower temperature = more focused, predictable responses\")\n",
    "print(\"   - Higher temperature = more creative, diverse responses\")\n",
    "print(\"   - Lower top-k = more conservative word choices\")\n",
    "print(\"   - More beams = potentially better quality but slower\")\n",
    "print(\"   - Greedy = fast but repetitive\")\n",
    "print(\"   - Sampling = creative but sometimes incoherent\")\n",
    "print(\"   - Beam search = balanced quality and coherence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Quick Chat Examples\n",
    "# Try these examples to see KyleBot in action\n",
    "\n",
    "print(\"üöÄ Quick Chat Examples\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Example 1: Simple question\n",
    "print(\"\\nüë§ You: What is machine learning?\")\n",
    "response = kylebot.generate_response(\"What is machine learning?\", method=\"sampling\", temperature=0.7)\n",
    "print(f\"ü§ñ KyleBot: {response}\")\n",
    "\n",
    "# Example 2: Creative prompt\n",
    "print(\"\\nüë§ You: Write a short story about a robot\")\n",
    "response = kylebot.generate_response(\"Write a short story about a robot\", method=\"sampling\", temperature=1.0, max_new_tokens=50)\n",
    "print(f\"ü§ñ KyleBot: {response}\")\n",
    "\n",
    "# Example 3: Technical question\n",
    "print(\"\\nüë§ You: How do neural networks work?\")\n",
    "response = kylebot.generate_response(\"How do neural networks work?\", method=\"beam\", num_beams=5, max_new_tokens=50)\n",
    "print(f\"ü§ñ KyleBot: {response}\")\n",
    "\n",
    "print(\"\\nüí° Now try your own questions! Use the chat function above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì What You've Learned\n",
    "\n",
    "Congratulations! You've built a complete generative AI chatbot. Here's what you now understand:\n",
    "\n",
    "### ü§ñ **Model Loading & Setup**\n",
    "- How to load pre-trained language models\n",
    "- Tokenization (converting text to numbers)\n",
    "- Model configuration and parameters\n",
    "\n",
    "### üß† **Text Generation Strategies**\n",
    "- **Greedy Decoding**: Always picks the most likely next word (fast, predictable)\n",
    "- **Sampling**: Randomly selects from likely words (creative, diverse)\n",
    "- **Beam Search**: Explores multiple possible sequences (balanced quality)\n",
    "\n",
    "### ‚öôÔ∏è **Key Parameters**\n",
    "- **Temperature**: Controls randomness (0.1 = focused, 1.5 = creative)\n",
    "- **Top-k**: Limits word choices to top k most likely\n",
    "- **Max Length**: Controls response length\n",
    "- **Num Beams**: Number of parallel searches in beam search\n",
    "\n",
    "### üèóÔ∏è **Software Architecture**\n",
    "- Object-oriented design with the KyleBot class\n",
    "- Conversation history management\n",
    "- Interactive user interface\n",
    "- Error handling and user commands\n",
    "\n",
    "### üöÄ **Next Steps to Explore**\n",
    "1. **Fine-tuning**: Train the model on your own data\n",
    "2. **Different Models**: Try GPT-3, BERT, or other models\n",
    "3. **Web Interface**: Build a web app for your chatbot\n",
    "4. **Memory**: Add long-term conversation memory\n",
    "5. **Personality**: Customize the bot's responses\n",
    "6. **Multi-turn**: Handle complex conversations\n",
    "\n",
    "### üí° **Pro Tips**\n",
    "- Start with sampling (temperature 0.7-0.9) for most use cases\n",
    "- Use beam search for factual or technical responses\n",
    "- Greedy decoding is good for simple, predictable tasks\n",
    "- Always clean and format your responses\n",
    "- Keep conversation history for context\n",
    "\n",
    "Happy learning! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
